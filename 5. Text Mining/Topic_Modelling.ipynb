{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News Aggregator\n",
    "\n",
    "## NLP: Topic Modelling\n",
    "\n",
    "### Keys Concepts\n",
    "- Latent Dirichlet Allocation\n",
    "- Latent Semantic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import seaborn as sns\n",
    "\n",
    "# sklearn for feature extraction & modeling\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.externals import joblib\n",
    "import joblib\n",
    "\n",
    "# Iteratively read files\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# For displaying images in ipython\n",
    "from IPython.display import HTML, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (14.0, 8.7)\n",
    "#warnings.filterwarnings('ignore')\n",
    "pd.options.display.float_format = '{:,.2f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User defined function to read and store bbc data from multipe folders\n",
    "def load_data(folder_names,root_path):\n",
    "    fileNames = [path + '/' + 'bbc' +'/'+ folder + '/*.txt' for path,folder in zip([root_path]*len(folder_names),\n",
    "                                                                               folder_names )]\n",
    "    doc_list = []\n",
    "    tags = folder_names\n",
    "    for docs in fileNames:\n",
    "        #print(docs)\n",
    "        #print(type(docs))\n",
    "        doc = glob.glob(docs) # glob method iterates through the all the text documents in a folder\n",
    "        for text in doc:\n",
    "            with open(text, encoding='latin1') as f:\n",
    "                topic = docs.split('/')[-2]\n",
    "\n",
    "                lines = f.readlines()\n",
    "                heading = lines[0].strip()\n",
    "                body = ' '.join([l.strip() for l in lines[1:]])\n",
    "                doc_list.append([topic, heading, body])\n",
    "        print(\"Completed loading data from folder: %s\"%topic)\n",
    "    \n",
    "    print(\"Completed Loading entire text\")\n",
    "    \n",
    "    return doc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed loading data from folder: business\n",
      "Completed loading data from folder: entertainment\n",
      "Completed loading data from folder: politics\n",
      "Completed loading data from folder: sport\n",
      "Completed loading data from folder: tech\n",
      "Completed Loading entire text\n"
     ]
    }
   ],
   "source": [
    "folder_names = ['business','entertainment','politics','sport','tech']\n",
    "docs = load_data(folder_names = folder_names, root_path = os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Category                            Heading  \\\n",
      "0  business    UK economy facing 'major risks'   \n",
      "1  business  Aids and climate top Davos agenda   \n",
      "2  business   Asian quake hits European shares   \n",
      "3  business   India power shares jump on debut   \n",
      "4  business    Lacroix label bought by US firm   \n",
      "\n",
      "                                             Article  \n",
      "0   The UK manufacturing sector will continue to ...  \n",
      "1   Climate change and the fight against Aids are...  \n",
      "2   Shares in Europe's leading reinsurers and tra...  \n",
      "3   Shares in India's largest power producer, Nat...  \n",
      "4   Luxury goods group LVMH has sold its loss-mak...  \n",
      "\n",
      "Shape of data is (2225, 3)\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2225 entries, 0 to 2224\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Category  2225 non-null   object\n",
      " 1   Heading   2225 non-null   object\n",
      " 2   Article   2225 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 52.3+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "docs = pd.DataFrame(docs, columns=['Category', 'Heading', 'Article'])\n",
    "print(docs.head())\n",
    "print('\\nShape of data is {}\\n'.format(docs.shape))\n",
    "print(docs.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Latent Dirichlet Allocation</h2>\n",
    "<h3>From Documents -- DTM -- LDA Model</h3>\n",
    "\n",
    "Topic modeling aims to automatically summarize large collections of documents to facilitate organization and management, as well as search and recommendations. At the same time, it can enable the understanding of documents to the extent that humans can interpret the descriptions of topics\n",
    "\n",
    "<img src=\"images/lda2.png\" alt=\"lda\" style=\"width:60%\">\n",
    "<img src=\"images/docs_to_lda.png\" alt=\"ldaflow\" style=\"width:100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Topics in Documents and group them: First Step to Build News Aggregator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Raw text --> Parsed Text --> Document Term Matrix --> Topic Models --> Extract Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Topic Modelling Works\n",
    "\n",
    "LDA  assumes topics are probability distributions over words, and documents are distributions over topics. Which implies that documents cover only a small set of topics, and topics use only a small set of words frequently.\n",
    "\n",
    "__The Mathematical Formula:__\n",
    "\n",
    "<img src=\"images/ldaformula.png\" alt=\"ldaflow\" style=\"width:100%\">\n",
    "\n",
    "\n",
    "### Important Reading Article on Topic Models\n",
    "https://medium.com/@lettier/how-does-lda-work-ill-explain-using-emoji-108abf40fa7d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Data to train & test Topic Models\n",
    "train_docs, test_docs = train_test_split(docs, \n",
    "                                         stratify=docs.Category, \n",
    "                                         test_size=50, \n",
    "                                         random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docs.shape, test_docs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=.2, \n",
    "                             min_df=.01, \n",
    "                             stop_words='english')\n",
    "\n",
    "train_dtm = vectorizer.fit_transform(train_docs[\"Article\"])\n",
    "words = vectorizer.get_feature_names()\n",
    "train_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign Number of Topics to be extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 5\n",
    "topic_labels = ['Topic {}'.format(i) for i in range(1, n_components+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_base = LatentDirichletAllocation(n_components=n_components,\n",
    "                                     n_jobs=-1,\n",
    "                                     learning_method='batch',\n",
    "                                     max_iter=10)\n",
    "lda_base.fit(train_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model on disk\n",
    "joblib.dump(lda_base,'lda_10_iter.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore topics & word distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pseudo counts\n",
    "topics_count = lda_base.components_\n",
    "print(topics_count.shape)\n",
    "topics_count[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_prob = topics_count / topics_count.sum(axis=1).reshape(-1, 1)\n",
    "topics = pd.DataFrame(topics_prob.T,\n",
    "                      index=words,\n",
    "                      columns=topic_labels)\n",
    "topics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(topics, cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = {}\n",
    "for topic, words_ in topics.items():\n",
    "    top_words[topic] = words_.nlargest(20).index.tolist()\n",
    "pd.DataFrame(top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Fit on Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = lda_base.transform(train_dtm)\n",
    "train_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = train_preds[0:5]\n",
    "print(t)\n",
    "np.argmax(t,axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_eval = pd.DataFrame(train_preds, columns=topic_labels, index=train_docs.Category)\n",
    "train_eval.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docs.loc[1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_eval.groupby(level='Category').mean().plot.bar(title='Avg. Topic Probabilities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train_eval.groupby(level='Category').idxmax(\n",
    "    axis=1).reset_index(-1, drop=True)\n",
    "sns.heatmap(df.groupby(level='Category').value_counts(normalize=True)\n",
    "            .unstack(-1), annot=True, fmt='.1%', cmap='Blues', square=True)\n",
    "plt.title('Train Data: Topic Assignments')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Fit on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dtm = vectorizer.transform(test_docs.Article)\n",
    "test_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = lda_base.transform(test_dtm)\n",
    "test_eval = pd.DataFrame(test_preds, columns=topic_labels, index=test_docs.Category)\n",
    "test_eval.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = test_eval.groupby(level='Category').idxmax(\n",
    "    axis=1).reset_index(-1, drop=True)\n",
    "sns.heatmap(df.groupby(level='Category').value_counts(normalize=True)\n",
    "            .unstack(-1), annot=True, fmt='.1%', cmap='Blues', square=True)\n",
    "plt.title('Topic Assignments');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Training Dataframe for Comparing Misclassified documents\n",
    "train_opt_eval = pd.DataFrame(data=lda_base.transform(train_dtm),\n",
    "                              columns=topic_labels,\n",
    "                              index=train_docs.Category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_opt_eval = pd.DataFrame(data=lda_base.transform(test_dtm),\n",
    "                              columns=topic_labels,\n",
    "                              index=test_docs.Category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Comparing Side by Side\n",
    "fig, axes = plt.subplots(ncols=2)\n",
    "source = ['Train', 'Test']\n",
    "for i, df in enumerate([train_opt_eval, test_opt_eval]):\n",
    "    df = df.groupby(level='Category').idxmax(\n",
    "    axis=1).reset_index(-1, drop=True)\n",
    "    sns.heatmap(df.groupby(level='Category').value_counts(normalize=True)\n",
    "            .unstack(-1), annot=True, fmt='.1%', cmap='Blues', square=True, ax=axes[i])\n",
    "    axes[i].set_title('{} Data: Topic Assignments'.format(source[i]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Misclassified Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_assignments = test_opt_eval.groupby(level='Category').idxmax(axis=1)\n",
    "test_assignments = test_assignments.reset_index(-1, drop=True).to_frame('predicted').reset_index()\n",
    "test_assignments['heading'] = test_docs.Heading.values\n",
    "test_assignments['Article'] = test_docs.Article.values\n",
    "test_assignments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
